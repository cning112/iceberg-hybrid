### **设计方案：地理分布式、高可用的 Iceberg 部署架构**

**版本: 2.1**

**作者: Gemini (Reviewed by User)**

---

### 1. 概述与目标

本文档描述了一个旨在解决数据全球化挑战的、地理分布式、高可用的 Apache Iceberg 数据湖部署架构。

本设计的核心目标如下：

*   **地理分布式写入 (Geo-Distributed Writes)**: 允许位于任何地理位置的写入任务（如 Spark/Flink 作业）将数据写入其最近的存储系统，以实现最低的写入延迟。
*   **地理分布式读取 (Geo-Distributed Reads)**: 保证位于任何地理位置的查询引擎（如 Trino/Presto）都能从一个完整的、本地化的数据副本中读取数据，以实现最低的查询延迟和数据出口成本。
*   **高可用性 (High Availability)**: 系统的核心元数据服务具有高可用性，并为数据同步和读取路径提供故障隔离。
*   **明确的一致性模型**: 为写入路径提供**强一致性**，为跨地域的读取副本提供**最终一致性**，并明确定义数据可见性的SLA。

---

### 2. 架构总览

本架构的核心思想是将**元数据管理（控制平面）**与**数据同步（数据平面）**彻底分离。

*   **控制平面**: 一个高可用的**主目录服务 (Primary Catalog)**，负责接收所有写入事务。该目录的变更通过异步复制，同步到其他地区的只读副本。
*   **数据平面**: 一个去中心化的**同步服务网格**，负责物理数据文件的拉取和元数据的本地化。

```
                                  ┌──────────────────────────────────┐
                                  │      Global Transactional        │
                                  │ Catalog (Nessie + Hybrid HA Backend)│
                                  └──────────────────┬─────────────────┘
                                                     │ (Commits to 'main' branch)
                                                     │
                         ┌───────────────────────────┴───────────────────────────┐
                         │ (Write jobs commit metadata with absolute, hetero URIs) │
                         ▼                                                       ▼
┌──────────────────────────────────┐                          ┌──────────────────────────────────┐
│        Location: US-East         │                          │        Location: EU-Central      │
│                                  │                          │                                  │
│  [Write/Query Engines]           │                          │  [Write/Query Engines]           │
│           │                      │                          │           │                      │
│           ▼                      │                          │           ▼                      │
│  ┌───────────────────┐           │                          │  ┌───────────────────┐           │
│  │   S3 Storage      │◄─────────┼──────────────────────────┼──►│   S3 Storage      │           │
│  │ (us-east-1)       │  (Sync)   │                          │  │ (eu-central-1)    │           │
│  └───────────────────┘           │                          │  └───────────────────┘           │
│           ▲                      │                          │           ▲                      │
│           │ (Reads from local)   │                          │           │ (Reads from local)   │
│           │                      │                          │           │                      │
│  ┌───────────────────┐           │                          │  ┌───────────────────┐           │
│  │  Sync Service     ├───────────┘                          └───────────┤  Sync Service     │           │
│  │  (Airflow + Rclone)│                                                │  (Airflow + Rclone)│           │
│  └───────────────────┘                                                └───────────────────┘           │
│           │                                                                     │                      │
│           └────────────► Reads 'main', Writes 'main_replica_us' ◄────────────┘                      │
│                                                                                                      │
└──────────────────────────────────┘                          └──────────────────────────────────┘
```

---

### 3. 核心组件详述

#### 3.1. 全局事务性目录

*   **服务层**: **Project Nessie**。
*   **后端实现 (核心挑战与权衡)**:
    *   **架构模式**: 为了在性能、稳定性、可用性和运维成本之间取得平衡，本设计推荐采用**主从异步复制**的数据库架构。
    *   **主集群 (Primary Cluster)**: 在一个主要地点（例如，美东）部署一个内部高可用的数据库集群（推荐 **CockroachDB** 或 **PostgreSQL with Patroni/Cloud HA**）。**所有 Nessie 的写入操作都只发往这个主集群**，从而保证写入路径的强一致性。
    *   **副本集群 (Replica Clusters)**: 在其他每个地区，部署一个只读的数据库副本。
    *   **复制机制**: 使用数据库层面的**异步复制**功能（例如，PostgreSQL 的流式复制或 CockroachDB 的 CDC 功能）来将主集群的数据同步到各地的副本集群。
*   **一致性语义澄清**:
    *   **写路径**: 在主集群所在地区，提供**强一致性**保证。
    *   **读路径**: 在副本集群所在地区，提供**最终一致性**。数据同步延迟（`replication_lag`）是一个核心的SLO监控指标。
*   **故障转移 (Failover)**:
    *   主集群的故障转移是一个重大的运维操作。本设计初期推荐采用**手动的、有计划的故障转移**流程，通过运维手册和脚本执行。这包括将一个副本集群提升为主、修改Nessie配置、以及更新DNS或服务发现记录。自动故障转移在广域网上风险极高，容易导致“脑裂”。
*   **分支策略**:
    *   `main` 分支: 主分支，在主集群上进行提交，包含异构URI。
    *   `main_replica_us`, `main_replica_eur` 等副本分支: 每个地区一个，由各地的同步服务在**本地的目录副本**上更新，只包含本地化路径。

#### 3.2. 同步服务

*   **角色**: 核心职责分为：1) **差分数据同步**；2) **元数据本地化**；3) **垃圾回收（GC）协同**。
*   **实现**: 推荐使用 **Apache Airflow** 作为编排器。每个地区的 Airflow 自身也应配置为高可用模式（例如，多节点、CeleryExecutor），以避免其成为本地同步任务的单点故障。
*   **关键特性**:
    *   **差分复制**: 必须通过 Iceberg API 计算快照间的差异，只复制新增的文件，而非全量比对。
    *   **快速前滚 (Fast-Forward)**: 当副本延迟过大时，应能跳过中间版本，直接同步到最新的快照，以尽快追平。
    *   **回压机制**: 同步服务应监控自身的处理能力和延迟。当延迟超过阈值时，应能发出告警，并考虑向上游（如果可能）或运维人员发出“减速”信号。

#### 3.3. 数据移动器

*   **角色**: 由同步服务调用，负责物理文件的跨云、跨区域复制。
*   **实现**: 推荐使用 **Rclone**。它功能强大，支持所有主流云存储，且其 `copy` 操作是幂等的，非常适合此场景。

#### 3.4. 存储注册表 (Storage Registry)

*   **角色**: 一个全局配置，用于将存储路径的前缀映射到访问该存储所需的元数据（如 Rclone 远程配置名、凭证ID等）。这是同步服务能够访问其他地区存储的关键。
*   **实现**: 一个由所有同步服务共享的 JSON 或 YAML 配置文件。

---

### 4. 关键工作流

#### 4.1. 写入工作流

1.  一个位于欧洲的 Spark 作业完成数据处理。
2.  它将生成的 Parquet 文件写入其最近的存储，即欧洲 S3 (`s3://my-company-eur/...`)。
3.  它向全局 Nessie 服务的 `main` 分支提交一个新的 commit，该 commit 的元数据中包含了新文件的完整绝对路径。

#### 4.2. 同步工作流 (以“美国同步服务”为例)

位于美国的 Airflow 同步服务按计划触发 DAG，其工作流严格分为两个连续的阶段：首先是数据同步，然后是元数据同步。

##### 4.2.1. 阶段一：数据同步 (物理文件层)

此阶段的目标是确保所有被全局 `main` 分支引用的数据文件，在本地存储中都存在一个物理副本。

1.  **计算差异**: 任务通过 Nessie 和 Iceberg 的 API，计算出 `main` 分支上自上次同步以来新增的**数据文件**的精确列表。
2.  **识别异地文件**: 遍历该列表，通过查询“存储注册表”，识别出所有文件路径不属于美国本地存储的“异地文件”。
3.  **执行物理复制**: 为每一个需要复制的异地文件，生成一个并行的、幂等的 `rclone` 任务，将其从源头（如欧洲S3）复制到本地的美国S3中。
4.  **确认完成**: 只有当一个批次（例如一个Nessie commit）所需的所有异地数据文件都被成功复制到本地后，才进入下一阶段。

##### 4.2.2. 阶段二：元数据同步与本地化 (逻辑层)

此阶段的目标是为本地查询引擎提供一个路径完全本地化的、可供查询的元数据入口点。

1.  **触发条件**: 对应批次的数据同步任务全部成功完成。
2.  **检出全局元数据**: 从 `main` 分支最新的、已完成数据同步的 commit 中，检出其完整的 Iceberg 元数据树。
3.  **路径重写**: 在内存中遍历元数据树（特别是 manifest 文件），将其中所有的文件 URI 都重写为指向美国本地 S3 的路径。
4.  **提交本地化元数据**: 将这份被完全“本地化”了的元数据，作为一个新的 commit，提交到美国地区的副本分支 `main_replica_us` 上。

完成此阶段后，位于美国的查询引擎就能通过 `main_replica_us` 分支安全、高效地查询数据了。

#### 4.3. 读取工作流 (以“美国查询引擎”为例)

1.  位于美国的 Trino 引擎被配置为从 Nessie 的 `main_replica_us` 分支读取数据。
2.  当用户发起查询时，Trino 从 Nessie 获取的所有元数据中，文件路径都已经是本地的 `s3://my-company-us/...` 路径。
3.  Trino 高效地从本地存储读取数据，完成查询。

---

### 5. 运维与成本

#### 5.1. 垃圾回收 (GC) 协同

1.  **GC执行**: `expire_snapshots` 等清理操作**只能在主目录集群**上执行。
2.  **候删清单**: 主目录在GC后，会生成一份“全局候删清单”，其中包含不再被任何活跃快照引用的数据文件URI。
3.  **清单分发**: 这份清单被视为一种特殊的元数据，通过数据库的异步复制，分发到所有副本目录集群。
4.  **安全删除**: 每个地区的同步服务，在消费到这份清单后，会引入一个**安全延迟窗口**（例如7天），确保没有正在运行的长查询会引用这些文件。之后，它才会从**本地存储副本**中删除这些物理文件。

#### 5.2. 监控与告警指标

*   **目录层**:
    *   `catalog_db_replication_lag_seconds`: 主从数据库之间的异步复制延迟。
    *   `nessie_commit_rate`: `main` 分支的提交速率。
    *   `nessie_api_latency_ms`: Nessie API 的响应延迟。
*   **同步服务层**:
    *   `sync_job_end_to_end_lag_seconds`: 从一个 commit 在 `main` 分支上发生，到其数据和元数据在本地副本完全可用的端到端延迟。
    *   `rclone_throughput_bytes_per_sec`: 数据移动的吞吐量。
    *   `num_files_to_sync`: 每个同步周期需要处理的文件数。
*   **成本层**:
    *   `cross_region_data_transfer_gb`: 跨区域数据传输量。
    *   `storage_api_call_count`: 存储API的调用次数。

#### 5.3. 成本优化

*   **清单服务**: 对于源和目标存储，都应启用清单服务（如 S3 Inventory），同步服务可以利用这份清单来避免高成本的 `LIST` 操作。
*   **小文件合并 (Compaction)**: 必须在写入端实施积极的 Compaction 策略，将小文件合并为更大的、适合分析的文件（例如256MB-1GB），这能极大地降低需要复制的文件数量和API开销。

---

### 6. 实施与迁移

#### 6.1. 冷启动 (Bootstrapping)

对于一个已存在的、PB级别的大表，首次在新的地区创建副本时，不能依赖网络复制。
1.  **物理迁移**: 使用 **AWS Snowball** 或 **GCP Transfer Appliance** 等物理设备，将源存储的全量数据一次性运送到新地区的数据中心。
2.  **数据加载**: 将数据从设备加载到新地区的本地存储中。
3.  **元数据初始化**: 在完成数据加载后，执行一次特殊的“全量”元数据同步和本地化任务，在新地区的副本分支上创建第一个快照。
4.  **切换到增量**: 初始化完成后，同步服务切换到正常的、基于增量 commit 的同步模式。

#### 6.2. 安全与网络

*   **网络**: 不同云或本地机房与云之间的通信，必须通过安全的 **VPN** 或 **专线（Direct Connect / Interconnect）**。VPC/VNet 的网络ACL和安全组必须经过严格配置，只允许必要的服务端口通信。
*   **身份与权限 (IAM)**:
    *   每个同步服务都应使用一个拥有**最小权限**的 IAM 角色或服务账户。
    *   例如，美东的同步服务，其角色应只拥有“读欧洲S3”和“写美东S3”的权限，以及访问Nessie和本地目录数据库的权限。
    *   凭证应通过安全的密钥管理服务（如 AWS Secrets Manager）进行管理和轮换。

---

### 7. 附录：与初始设计的对比分析

#### 核心思想演进

我们的讨论始于一个**主从复制（Primary-Replica）**模型，其目标是在云端创建一个高可靠的**只读镜像**，这是一个典型的**灾难恢复（Disaster Recovery）**方案。

经过层层深入的探讨，我们最终演进到了一个**对等网络（Peer-to-Peer）**模型，其目标是支持**全球化的读写**，这是一个真正的**高可用（High Availability）**架构。

#### 详细差异对比

| 特性 / 组件 | 初始设计 (`iceberg-arch-hybrid-replica-dr.md`) | 演进后的设计 (`iceberg-arch-geo-distributed-ha.md`) | 差异与演进原因 |
| :--- | :--- | :--- | :--- |
| **核心架构** | 主从复制 (本地写，云端读) | 地理分布式 (全球任意地点可读写) | **目标升级**: 从简单的灾难恢复，升级为支持全球化的业务需求。 |
| **写入能力** | **中心化**: 只有本地的 SoT (单一真相源) 能接受写入。 | **去中心化**: 任何地点的任务都可以写入其本地存储。 | 为了给全球各地的写入任务提供最低的延迟。 |
| **目录服务** | 本地单一数据库 (Postgres) + 云端只读目录 (BLMS)。 | **全局事务性目录** (Nessie + 分布式数据库)。 | 消除写入路径的单点故障，为全球写入提供一个统一、高可用的元数据真相源。 |
| **高可用性** | **读可用**: 是 (云端副本可用)。<br>**写可用**: **否** (本地 SoT 是单点故障)。 | **读写均高可用**: **是** (采用主从异步复制的目录后端，主集群故障时可手动切换)。 | 满足了真正的业务连续性要求，而不仅仅是数据备份。 |
| **数据同步模型** | **推送模型 (Push)**: 一个中心的 `Replicator` 将数据从本地**推送**到云端。 | **拉取模型 (Pull)**: 每个地区的 `Sync Service` 从其他地区**拉取**数据。 | 数据的来源不再是单一地点，因此每个节点都必须主动去同步其他节点的数据。 |
| **同步协调机制** | **显式协调**: 使用 `_inprogress` 和 `_ready` marker 文件来作为信号，通知下游的 `Follower` 服务。 | **隐式协调**: **不再需要 Marker 文件**。Nessie 的 commit 本身就是全局的、原子性的信号。 | 协调机制由应用层的、复杂的 Marker 文件逻辑，演进为目录服务内置的、更可靠的事务日志。 |
| **元数据处理** | 复制器物理复制元数据文件；Follower 更新一个独立的云端目录。 | 同步服务在拉取完数据后，生成一份**路径本地化**的元数据，并提交到 **Nessie 的副本分支**上。 | 为本地查询引擎提供一个干净、高效的读取视图，避免了引擎层改造的复杂性。 |
| **读取路径** | 云端引擎查询一个独立的、只读的目录副本 (BLMS)。 | 各地引擎查询同一个全局目录 (Nessie) 的**不同分支** (例如 `main_replica_us`)。 | 简化了系统的拓扑结构，利用了 Nessie 强大的分支能力来隔离读写。 |
| **架构复杂度** | 概念简单，但有多个独立的移动部件 (Replicator, Follower, Marker)，协调逻辑复杂。 | 核心概念更先进 (Raft, Nessie)，但一旦建立，系统运行时的移动部件更少，逻辑更内聚、更健壮。 | 将复杂性从“过程协调”转移到了“状态共识”，后者有更成熟的理论和工具支持。 |

#### 总结

总而言之，我们的设计从一个相对传统的、用于数据备份和灾难恢复的**主从架构**，演进成了一个采用现代分布式系统核心思想（如共识协议、控制与数据平面分离）的、真正意义上的**全球化、高可用架构**。

演进后的设计在**可用性、灵活性和可扩展性**上远超初始设计，虽然引入了 Nessie 等新组件，但通过消除脆弱的、手动的协调机制，反而让整个系统变得更加**健壮和逻辑自洽**。
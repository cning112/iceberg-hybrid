### **设计方案：地理分布式、高可用的 Iceberg 部署架构**

**版本: 1.2**

**作者: Gemini**

---

### 1. 概述与目标

本文档描述了一个旨在解决数据全球化挑战的、地理分布式、高可用的 Apache Iceberg 数据湖部署架构。随着业务遍布全球，数据读写需要在不同地理位置（数据中心或云区域）进行，同时要保证数据的一致性和查询性能。

本设计的核心目标如下：

*   **地理分布式写入 (Geo-Distributed Writes)**: 允许位于任何地理位置的写入任务（如 Spark/Flink 作业）将数据写入其最近的存储系统，以实现最低的写入延迟。
*   **地理分布式读取 (Geo-Distributed Reads)**: 保证位于任何地理位置的查询引擎（如 Trino/Presto）都能从一个完整的、本地化的数据副本中读取数据，以实现最低的查询延迟和数据出口成本。
*   **高可用性 (High Availability)**: 整个系统的写入和读取路径都没有单点故障，能够容忍单个节点甚至整个数据中心的故障。
*   **强一致性 (Strong Consistency)**: 所有的数据变更都必须是事务性的，保证数据的完整和正确，绝不允许出现数据不一致或损坏的情况。

---

### 2. 架构总览

本架构的核心思想是将**元数据管理（控制平面）**与**数据同步（数据平面）**彻底分离。

```
                                  ┌──────────────────────────────────┐
                                  │      Global Transactional        │
                                  │ Catalog (Nessie + Hybrid HA Backend)│
                                  └──────────────────┬─────────────────┘
                                                     │ (Commits to 'main' branch)
                                                     │
                         ┌───────────────────────────┴───────────────────────────┐
                         │ (Write jobs commit metadata with absolute, hetero URIs) │
                         ▼                                                       ▼
┌──────────────────────────────────┐                          ┌──────────────────────────────────┐
│        Location: US-East         │                          │        Location: EU-Central      │
│                                  │                          │                                  │
│  [Write/Query Engines]           │                          │  [Write/Query Engines]           │
│           │                      │                          │           │                      │
│           ▼                      │                          │           ▼                      │
│  ┌───────────────────┐           │                          │  ┌───────────────────┐           │
│  │   S3 Storage      │◄─────────┼──────────────────────────┼──►│   S3 Storage      │           │
│  │ (us-east-1)       │  (Sync)   │                          │  │ (eu-central-1)    │           │
│  └───────────────────┘           │                          │  └───────────────────┘           │
│           ▲                      │                          │           ▲                      │
│           │ (Reads from local)   │                          │           │ (Reads from local)   │
│           │                      │                          │           │                      │
│  ┌───────────────────┐           │                          │  ┌───────────────────┐           │
│  │  Sync Service     ├───────────┘                          └───────────┤  Sync Service     │           │
│  │  (Airflow + Rclone)│                                                │  (Airflow + Rclone)│           │
│  └───────────────────┘                                                └───────────────────┘           │
│           │                                                                     │                      │
│           └────────────► Reads 'main', Writes 'main_replica_us' ◄────────────┘                      │
│                                                                                                      │
└──────────────────────────────────┘                          └──────────────────────────────────┘
```

**核心组件**:
*   **全局事务性目录 (Global Transactional Catalog)**: 整个系统的“大脑”，采用 **Project Nessie** 作为服务层，并由一个**支持混合云部署的高可用数据库**作为后端。它是所有元数据变更的唯一真相源。
*   **分布式存储 (Distributed Storage)**: 位于不同地理位置的多个对象存储系统，例如 AWS S3, Google GCS 等。
*   **同步服务网格 (Synchronization Service Mesh)**: 一个去中心化的服务网络。在每个地理位置都部署一个独立的同步服务实例，负责将其他地区的数据拉取到本地，并维护一个本地化的元数据副本。
*   **查询与写入引擎**: 标准的 Spark, Flink, Trino 等引擎，无需任何改造。

---

### 3. 核心组件详述

#### 3.1. 全局事务性目录

*   **服务层**: **Project Nessie**。
*   **后端实现 (核心挑战)**: 为 Nessie 提供一个跨本地机房和多云的高可用后端，是本架构中最具挑战性的部分。必须仔细权衡和选择。
    *   **云原生方案的局限性**: 像 **Amazon DynamoDB** 或 **Google Bigtable** 这样的服务，可以在其各自的云生态系统内提供出色的、易于管理的跨区域高可用性。但它们**无法满足**跨云或混合云（本地+云）的部署要求。
    *   **理论上的理想方案**: 部署一个**单一的、横跨本地机房和多个云的 CockroachDB 集群**。这在理论上能提供最强的HA，但实践中会面临巨大的网络延迟、稳定性和运维复杂性挑战，可能导致性能和可靠性下降，通常被认为是一种“反模式”。
    *   **务实的灾备方案 (推荐)**: 承认广域网的物理限制，采用**联邦式的、异步复制**的数据库架构。
        1.  在每个主要地点（如本地机房、AWS、GCP）都部署一个**独立的、内部高可用的数据库集群**（例如，一个高可用的Postgres集群或一个局限于该地点的CockroachDB集群）。
        2.  指定其中**一个集群作为主集群（Primary）**，Nessie 的写入流量只发往此集群。
        3.  采用数据库层面的**异步复制**功能（如CDC - Change Data Capture），将主集群的数据变更复制到其他地点的从集群。
        4.  这种方式在目录后端层面，回归到了一个健壮的“主从复制”灾难恢复模型。它牺牲了写入路径的自动全局故障转移，但换来了高性能、高稳定性和可管理性。
*   **分支策略**:
    *   `main` 分支: 主分支，作为全局写入的唯一入口。所有写入任务，无论在何地，都将其 commit 提交到此分支。此分支的元数据中包含指向各个不同存储系统的、完整的、异构的 URI。
    *   `main_replica_us`, `main_replica_eur` 等副本分支: 每个地区一个，只读。它们包含了对应地区**完全本地化**的元数据（所有文件路径都指向本地存储）。各地区的查询引擎只从这些分支读取。

#### 3.2. 同步服务

*   **角色**: 其核心职责分为两部分：1) **数据同步**：保证每个地区都拥有一份全量的数据文件副本。2) **元数据本地化**：为本地查询引擎提供一个只包含本地路径的、可查询的元数据视图。
*   **实现**: 推荐使用 **Apache Airflow** 作为编排器，并采用**完全去中心化**的部署模式（即每个地区一套独立的 Airflow），以实现最高的可用性和故障隔离。

#### 3.3. 数据移动器

*   **角色**: 由同步服务调用，负责物理文件的跨云、跨区域复制。
*   **实现**: 推荐使用 **Rclone**。它功能强大，支持所有主流云存储，且其 `copy` 操作是幂等的，非常适合此场景。

#### 3.4. 存储注册表 (Storage Registry)

*   **角色**: 一个全局配置，用于将存储路径的前缀映射到访问该存储所需的元数据（如 Rclone 远程配置名、凭证ID等）。这是同步服务能够访问其他地区存储的关键。
*   **实现**: 一个由所有同步服务共享的 JSON 或 YAML 配置文件。

---

### 4. 关键工作流

#### 4.1. 写入工作流

1.  一个位于欧洲的 Spark 作业完成数据处理。
2.  它将生成的 Parquet 文件写入其最近的存储，即欧洲 S3 (`s3://my-company-eur/...`)。
3.  它向全局 Nessie 服务的 `main` 分支提交一个新的 commit，该 commit 的元数据中包含了新文件的完整绝对路径。

#### 4.2. 同步工作流 (以“美国同步服务”为例)

位于美国的 Airflow 同步服务按计划触发 DAG，其工作流严格分为两个连续的阶段：首先是数据同步，然后是元数据同步。

##### 4.2.1. 阶段一：数据同步 (物理文件层)

此阶段的目标是确保所有被全局 `main` 分支引用的数据文件，在本地存储中都存在一个物理副本。

1.  **计算差异**: 任务通过 Nessie 和 Iceberg 的 API，计算出 `main` 分支上自上次同步以来新增的**数据文件**的精确列表。
2.  **识别异地文件**: 遍历该列表，通过查询“存储注册表”，识别出所有文件路径不属于美国本地存储的“异地文件”。
3.  **执行物理复制**: 为每一个需要复制的异地文件，生成一个并行的、幂等的 `rclone` 任务，将其从源头（如欧洲S3）复制到本地的美国S3中。
4.  **确认完成**: 只有当一个批次（例如一个Nessie commit）所需的所有异地数据文件都被成功复制到本地后，才进入下一阶段。

##### 4.2.2. 阶段二：元数据同步与本地化 (逻辑层)

此阶段的目标是为本地查询引擎提供一个路径完全本地化的、可供查询的元数据入口点。

1.  **触发条件**: 对应批次的数据同步任务全部成功完成。
2.  **检出全局元数据**: 从 `main` 分支最新的、已完成数据同步的 commit 中，检出其完整的 Iceberg 元数据树。
3.  **路径重写**: 在内存中遍历元数据树（特别是 manifest 文件），将其中所有的文件 URI 都重写为指向美国本地 S3 的路径。
4.  **提交本地化元数据**: 将这份被完全“本地化”了的元数据，作为一个新的 commit，提交到美国地区的副本分支 `main_replica_us` 上。

完成此阶段后，位于美国的查询引擎就能通过 `main_replica_us` 分支安全、高效地查询数据了。

#### 4.3. 读取工作流 (以“美国查询引擎”为例)

1.  位于美国的 Trino 引擎被配置为从 Nessie 的 `main_replica_us` 分支读取数据。
2.  当用户发起查询时，Trino 从 Nessie 获取的所有元数据中，文件路径都已经是本地的 `s3://my-company-us/...` 路径。
3.  Trino 高效地从本地存储读取数据，完成查询。

---

### 5. 结论

本设计通过将**元数据一致性（由 Nessie + 分布式数据库保证）**与**物理数据复制（由分布式的同步服务保证）**相分离，构建了一个健壮、解耦的系统。它成功地将一个复杂的海量异构数据复制问题，分解为了一个已解决的元数据共识问题和一个可并行化的数据搬运问题，最终实现了地理分布式读写、高可用和强一致性的设计目标。

---

### 6. 附录：与初始设计的对比分析

#### 核心思想演进

我们的讨论始于一个**主从复制（Primary-Replica）**模型，其目标是在云端创建一个高可靠的**只读镜像**，这是一个典型的**灾难恢复（Disaster Recovery）**方案。

经过层层深入的探讨，我们最终演进到了一个**对等网络（Peer-to-Peer）**模型，其目标是支持**全球化的读写**，这是一个真正的**高可用（High Availability）**架构。

#### 详细差异对比

| 特性 / 组件 | 初始设计 (`iceberg-arch-hybrid-replica-dr.md`) | 演进后的设计 (`HA_Icerberg.md`) | 差异与演进原因 |
| :--- | :--- | :--- | :--- |
| **核心架构** | 主从复制 (本地写，云端读) | 地理分布式 (全球任意地点可读写) | **目标升级**: 从简单的灾难恢复，升级为支持全球化的业务需求。 |
| **写入能力** | **中心化**: 只有本地的 SoT (单一真相源) 能接受写入。 | **去中心化**: 任何地点的任务都可以写入其本地存储。 | 为了给全球各地的写入任务提供最低的延迟。 |
| **目录服务** | 本地单一数据库 (Postgres) + 云端只读目录 (BLMS)。 | **全局事务性目录** (Nessie + 分布式数据库)。 | 消除写入路径的单点故障，为全球写入提供一个统一、高可用的元数据真相源。 |
| **高可用性** | **读可用**: 是 (云端副本可用)。<br>**写可用**: **否** (本地 SoT 是单点故障)。 | **读写均高可用**: **是**。可容忍任何单个地点的故障。 | 满足了真正的业务连续性要求，而不仅仅是数据备份。 |
| **数据同步模型** | **推送模型 (Push)**: 一个中心的 `Replicator` 将数据从本地**推送**到云端。 | **拉取模型 (Pull)**: 每个地区的 `Sync Service` 从其他地区**拉取**数据。 | 数据的来源不再是单一地点，因此每个节点都必须主动去同步其他节点的数据。 |
| **同步协调机制** | **显式协调**: 使用 `_inprogress` 和 `_ready` marker 文件来作为信号，通知下游的 `Follower` 服务。 | **隐式协调**: **不再需要 Marker 文件**。Nessie 的 commit 本身就是全局的、原子性的信号。 | 协调机制由应用层的、复杂的 Marker 文件逻辑，演进为目录服务内置的、更可靠的事务日志。 |
| **元数据处理** | 复制器物理复制元数据文件；Follower 更新一个独立的云端目录。 | 同步服务在拉取完数据后，生成一份**路径本地化**的元数据，并提交到 **Nessie 的副本分支**上。 | 为本地查询引擎提供一个干净、高效的读取视图，避免了引擎层改造的复杂性。 |
| **读取路径** | 云端引擎查询一个独立的、只读的目录副本 (BLMS)。 | 各地引擎查询同一个全局目录 (Nessie) 的**不同分支** (例如 `main_replica_us`)。 | 简化了系统的拓扑结构，利用了 Nessie 强大的分支能力来隔离读写。 |
| **架构复杂度** | 概念简单，但有多个独立的移动部件 (Replicator, Follower, Marker)，协调逻辑复杂。 | 核心概念更先进 (Raft, Nessie)，但一旦建立，系统运行时的移动部件更少，逻辑更内聚、更健壮。 | 将复杂性从“过程协调”转移到了“状态共识”，后者有更成熟的理论和工具支持。 |

#### 总结

总而言之，我们的设计从一个相对传统的、用于数据备份和灾难恢复的**主从架构**，演进成了一个采用现代分布式系统核心思想（如共识协议、控制与数据平面分离）的、真正意义上的**全球化、高可用架构**。

演进后的设计在**可用性、灵活性和可扩展性**上远超初始设计，虽然引入了 Nessie 等新组件，但通过消除脆弱的、手动的协调机制，反而让整个系统变得更加**健壮和逻辑自洽**。